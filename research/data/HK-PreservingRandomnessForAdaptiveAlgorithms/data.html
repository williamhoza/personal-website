<data value="TITLE">
  Preserving Randomness for Adaptive Algorithms
</data>

<data value="AUTHORS">
  William M. Hoza and Adam R. Klivans
</data>

<data value="ABBREV">
  HK
</data>

<data value="VENUE">RANDOM 2018</data>

<data value="PAPER-LINKS">
    <a href="https://arxiv.org/abs/1611.00783">arXiv</a>
    <a href="https://eccc.weizmann.ac.il/report/2016/172/">ECCC</a>
    <a href="https://doi.org/10.4230/LIPIcs.APPROX-RANDOM.2018.43">RANDOM proceedings</a>
</data>

<data value="VERSION-SUMMARY">
  We posted a manuscript online in November 2016; I presented the paper at RANDOM in August 2018. The arXiv version and the ECCC version are identical. The RANDOM proceedings version is more compact and skips a lot of the material.
</data>

<data value="FIRST-POSTED">
  2016-11-02
</data>

<data value="ABSTRACT">
  <p>
    Suppose $\mathsf{Est}$ is a randomized estimation algorithm that uses $n$ random bits and outputs values in $\mathbb{R}^d$. We show how to execute $\mathsf{Est}$ on $k$ adaptively chosen inputs using only $n + O(k \log(d + 1))$ random bits instead of the trivial $nk$ (at the cost of mild increases in the error and failure probability). Our algorithm combines a variant of the INW pseudorandom generator (STOC '94) with a new scheme for shifting and rounding the outputs of $\mathsf{Est}$. We prove that modifying the outputs of $\mathsf{Est}$ is necessary in this setting, and furthermore, our algorithm's randomness complexity is near-optimal in the case $d \leq O(1)$. As an application, we give a randomness-efficient version of the Goldreich-Levin algorithm; our algorithm finds all Fourier coefficients with absolute value at least $\theta$ of a function $F \colon \{0, 1\}^n \to \{-1, 1\}$ using $O(n \log n) \cdot \text{poly}(1/\theta)$ queries to $F$ and $O(n)$ random bits (independent of $\theta$), improving previous work by Bshouty et al. (JCSS '04).
  </p>  
</data>

<data value="NOT-SO-ABSTRACT">
  <p>
    Suppose you have a randomized algorithm that estimates some numeric quantity, and you plan to use it $k$ times. Suppose the estimation algorithm makes $n$ coin tosses. This situation seems to call for $nk$ coin tosses in total. In this paper, we show that you can actually get away with only making about $n + k$ coin tosses if you use some tricks.
  </p>  
</data>

<data value="EXPOSITORY">
  <a href="RANDOM-slides.pdf"><img src="RANDOM-slides.png"></a>
  <p>
    <a href="RANDOM-slides.pdf">Slides</a> from my presentation at RANDOM (August 2018). See also <a href="caltech-theory-seminar-slides.pdf">these slides</a> from my longer presentation at Caltech's CS Theory Seminar (May 2017). In both cases, I recommend viewing the slides in Adobe Reader to get the animations to work.
  </p>
  <iframe src="https://www.youtube.com/embed/4x4SCTALca8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  <p>
    <a href="https://youtu.be/4x4SCTALca8">Video</a> from Adam's presentation at the Simons Institute "Proving and Using Pseudorandomness" workshop (March 2017).
  </p>
</data>