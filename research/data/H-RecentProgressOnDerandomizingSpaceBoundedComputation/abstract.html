<p>
  Is randomness ever necessary for space-efficient computation? It is commonly conjectured that \(\mathsf{L} = \mathsf{BPL}\), meaning that halting decision algorithms can always be derandomized without increasing their space complexity by more than a constant factor. In the past few years (say, from 2017 to 2022), there has been some exciting progress toward proving this conjecture. Thanks to recent work, we have new pseudorandom generators (PRGs), new black-box derandomization algorithms (generalizations of PRGs), and new non-black-box derandomization algorithms. This article is a survey of these recent developments. We organize the underlying techniques into four overlapping themes:
</p>

<ol>
  <li>The <em>iterated pseudorandom restrictions</em> framework for designing PRGs, especially PRGs for functions computable by arbitrary-order read-once branching programs.</li>
  <li>The <em>inverse Laplacian</em> perspective on derandomizing \(\mathsf{BPL}\) and the related concept of local consistency.</li>
  <li><em>Error reduction</em> procedures, including methods of designing low-error weighted pseudorandom generators (WPRGs).</li>
  <li>The continued use of <em>spectral expander graphs</em> in this domain via the derandomized square operation and the Impagliazzo-Nisan-Wigderson PRG (STOC 1994).</li>
</ol>

<p>
  We give an overview of these ideas and their applications, and we discuss the challenges ahead.
</p>
